{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyreadstat\n",
    "\n",
    "from koafusion.datasets.oai import (prefix_var_to_visit_month,\n",
    "                                    release_to_visit_month,\n",
    "                                    side_code_to_str)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "DIR_PROJECT_ROOT = # TODO: set to project root directory\n",
    "DIR_DATA_ROOT = Path(DIR_PROJECT_ROOT, \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read the meta info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_compose_asmts(fpaths, capitalize=False, verbose=False):\n",
    "    \"\"\" \"\"\"\n",
    "    if verbose:\n",
    "        print(fpaths)\n",
    "    dfs = []\n",
    "\n",
    "    for i, fpath in enumerate(fpaths):\n",
    "        if fpath.suffix in (\".csv\", \".txt\"):\n",
    "            df = pd.read_csv(fpath, sep='|', index_col=False)\n",
    "        elif fpath.suffix == \".sas7bdat\":\n",
    "            df, _ = pyreadstat.read_sas7bdat(fpath, user_missing=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported extension: {fpath.suffix}\")\n",
    "        \n",
    "        if capitalize:\n",
    "            # Capitalize all columns names\n",
    "            df.columns = df.columns.str.upper()\n",
    "\n",
    "        # Find release info\n",
    "        prefix_var = 'VXX'\n",
    "        for c in df.columns:\n",
    "            if re.match(\"V\\d\\d.*$\", c):\n",
    "                prefix_var = c[:3]\n",
    "                break\n",
    "        # Remove prefix from column names and add corresponding column\n",
    "        columns = []\n",
    "        for c in df.columns:\n",
    "            if c.startswith(prefix_var):\n",
    "                columns.append(c[3:])\n",
    "            else:\n",
    "                columns.append(c)\n",
    "        df.columns = columns\n",
    "        df.loc[:, 'PREFIX_VAR'] = prefix_var\n",
    "\n",
    "        if verbose:\n",
    "            print(f'df idx: {i} num: {len(df)}')\n",
    "        dfs.append(df)\n",
    "\n",
    "    if len(dfs) > 1:\n",
    "        out = pd.concat(dfs, axis=0)\n",
    "    else:\n",
    "        out = dfs[0]\n",
    "    if verbose:\n",
    "        print(f\"num total: {len(out)}\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_compose_contents(paths):\n",
    "    dfs = []\n",
    "    for p in paths:\n",
    "        df = pd.read_csv(p, index_col=False, sep=\",\",\n",
    "                         dtype={\"Folder\": str,\n",
    "                                \"ParticipantID\": str,\n",
    "                                \"StudyDate\": int,\n",
    "                                \"SeriesDescription\": str})\n",
    "        df.loc[:, \"visit_month\"] = [release_to_visit_month[e.split(\"/\")[0]]\n",
    "                                    for e in df[\"Folder\"].tolist()]\n",
    "        df.loc[:, \"visit\"] = [int(p[:-1]) for p in df[\"visit_month\"].tolist()]\n",
    "        \n",
    "        df = df.rename(columns={\"ParticipantID\": \"patient\"})\n",
    "        \n",
    "        dfs.append(df)\n",
    "    out = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "    return out\n",
    "\n",
    "\n",
    "def preproc_contents(df):\n",
    "    # Preprocess imaging inventory for easier merging\n",
    "    df_t = df.copy()\n",
    "    df_t[\"sequence\"] = \"\"\n",
    "    df_t[\"side\"] = \"\"\n",
    "    \n",
    "    mapping = {\n",
    "        # series_in: (series_out, side)\n",
    "        'MP_LOCATOR_LEFT': ('MP_LOCATOR', \"LEFT\"),\n",
    "        'MP_LOCATOR_RIGHT': ('MP_LOCATOR', \"RIGHT\"),\n",
    "        'COR_IW_TSE_LEFT': ('COR_IW_TSE', \"LEFT\"),\n",
    "        'COR_IW_TSE_RIGHT': ('COR_IW_TSE', \"RIGHT\"),\n",
    "        'PA Fixed Flexion Left Knee': ('PA Fixed Flexion Knee', \"LEFT\"),\n",
    "        'PA Fixed Flexion Right Knee': ('PA Fixed Flexion Knee', \"RIGHT\"),\n",
    "        'SAG_T2_CALC_LEFT': ('SAG_T2_CALC', \"LEFT\"),\n",
    "        'SAG_T2_CALC_RIGHT': ('SAG_T2_CALC', \"RIGHT\"),\n",
    "        'SAG_3D_DESS_LEFT': ('SAG_3D_DESS', \"LEFT\"),\n",
    "        'SAG_3D_DESS_RIGHT': ('SAG_3D_DESS', \"RIGHT\"),\n",
    "        'COR_MPR_LEFT': ('COR_MPR', \"LEFT\"),\n",
    "        'COR_MPR_RIGHT': ('COR_MPR', \"RIGHT\"),\n",
    "        'AX_MPR_LEFT': ('AX_MPR', \"LEFT\"),\n",
    "        'AX_MPR_RIGHT': ('AX_MPR', \"RIGHT\"),\n",
    "        'SAG_IW_TSE_LEFT': ('SAG_IW_TSE', \"LEFT\"),\n",
    "        'SAG_IW_TSE_RIGHT': ('SAG_IW_TSE', \"RIGHT\"),\n",
    "        'COR_T1_3D_FLASH_LEFT': ('COR_T1_3D_FLASH', \"LEFT\"),\n",
    "        'COR_T1_3D_FLASH_RIGHT': ('COR_T1_3D_FLASH', \"RIGHT\"),\n",
    "        'SAG_T2_MAP_LEFT': ('SAG_T2_MAP', \"LEFT\"),\n",
    "        'SAG_T2_MAP_RIGHT': ('SAG_T2_MAP', \"RIGHT\"),\n",
    "        'Bilateral PA Fixed Flexion Knee': ('Bilateral PA Fixed Flexion Knee', \"OTHER\"),\n",
    "        'Full Limb': ('Full Limb', \"OTHER\"),\n",
    "        'MP_LOCATOR_THIGH': ('MP_LOCATOR_THIGH', \"OTHER\"),\n",
    "        'AX_T1_THIGH': ('AX_T1_THIGH', \"OTHER\"),\n",
    "        'PRESCRIPTION_THIGH': ('PRESCRIPTION_THIGH', \"OTHER\"),\n",
    "        'Lateral Left Knee': ('Lateral Knee', \"LEFT\"),\n",
    "        'Lateral Right Knee': ('Lateral Knee', \"RIGHT\"),\n",
    "        'AP Pelvis': ('AP Pelvis', \"OTHER\"),\n",
    "        'PA Left Hand': ('PA Hand', \"LEFT\"),\n",
    "        'PA Right Hand': ('PA Hand', \"RIGHT\"),\n",
    "        'PA Bilateral Hand': ('PA Bilateral Hand', \"OTHER\"),\n",
    "        'OTHER': ('OTHER', \"OTHER\"),\n",
    "    }\n",
    "\n",
    "    df_proc = df_t.assign(\n",
    "        sequence=lambda x: [mapping[e][0] for e in x[\"SeriesDescription\"].tolist()],\n",
    "        side=lambda x: [mapping[e][1] for e in x[\"SeriesDescription\"].tolist()]\n",
    "    )\n",
    "    return df_proc\n",
    "\n",
    "\n",
    "dir_contents = Path(DIR_DATA_ROOT, \"contents\")\n",
    "paths_contents = dir_contents.glob(\"*.csv\")\n",
    "\n",
    "df_contents = read_compose_contents(paths_contents)\n",
    "display(df_contents.head())\n",
    "\n",
    "df_contents_proc = preproc_contents(df_contents)\n",
    "display(df_contents_proc.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_compose_clinical(paths):\n",
    "    df = read_compose_asmts(paths)\n",
    "\n",
    "    df.columns = df.columns.str.upper()\n",
    "    df = df.rename(columns={\"ID\": \"patient\", })\n",
    "    \n",
    "    df = df.astype({\"patient\": str})\n",
    "    \n",
    "    df.loc[:, \"visit_month\"] = [prefix_var_to_visit_month[p]\n",
    "                                for p in df[\"PREFIX_VAR\"].tolist()]\n",
    "    df.loc[:, \"visit\"] = [int(p[:-1]) for p in df[\"visit_month\"].tolist()]\n",
    "\n",
    "    sel = [\n",
    "        \"patient\", \"PREFIX_VAR\", \"visit_month\", \"visit\",\n",
    "        \"AGE\", \"P01BMI\", \"LKDEFCV\", \"RKDEFCV\",\n",
    "        \"P01INJR\", \"P01INJR1\", \"P01INJR2\", \"P01INJR3\", \n",
    "        \"P01KSURGR\", \"P01KRSR\", \"P01KRSRA\", \n",
    "        \"P01ARTR\", \"P01ARTR1\", \"P01ARTR2\", \"P01ARTR3\", \"P01ARTRINJ\", \n",
    "        \"P01MENR\", \"P01MENR1\", \"P01MENR2\", \"P01MENR3\", \"P01MENRINJ\", \n",
    "        \"P01LRR\", \"P01LRR1\", \"P01LRR2\", \"P01LRR3\", \"P01OTSURGR\",\n",
    "        \"P01OTSR1\", \"P01OTSR2\", \"P01OTSR3\", \"P01OTSRINJ\",\n",
    "        \"P01INJL\", \"P01INJL1\", \"P01INJL2\", \"P01INJL3\", \n",
    "        \"P01KSURGL\", \"P01KRSL\", \"P01KRSLA\", \n",
    "        \"P01ARTL\", \"P01ARTL1\", \"P01ARTL2\", \"P01ARTL3\", \"P01ARTLINJ\", \n",
    "        \"P01MENL\", \"P01MENL1\", \"P01MENL2\", \"P01MENL3\", \"P01MENLINJ\", \n",
    "        \"P01LRL\", \"P01LRL1\", \"P01LRL2\", \"P01LRL3\", \n",
    "        \"P01OTSURGL\", \"P01OTSL1\", \"P01OTSL2\", \"P01OTSL3\", \"P01OTSLINJ\", \n",
    "        \"P01RHBE\", \"P01LHBE\", \"RKPFCRE\", \"LKPFCRE\", \n",
    "        \"WSRKN1\", \"WSRKN2\", \"WSLKN1\", \"WSLKN2\", \n",
    "        \"KOOSYMR\", \"KOOSYML\", \"KOOSKPL\", \"KOOSKPR\",\n",
    "        \"KPA30CV\", \"KPACDCV\", \"KPACT30\", \"KPACTCV\", \"KPMED\", \"KPMEDCV\", \n",
    "        \"KPNL12\", \"KPNR12\", \"KPNL12M\", \"KPNR12M\", \"KPL12CV\", \"KPR12CV\", \n",
    "        \"KPL30CV\", \"KPR30CV\", \"KPRK20B\", \"KPLK20B\", \"KPLK20D\", \"KPRK20D\",\n",
    "        \"KPLKN1\", \"KPLKN2\", \"KPLKN3\", \"KPRKN1\", \"KPRKN2\", \"KPRKN3\",\n",
    "        \"MISSWK\", \"PMLKRCV\", \"PMRKRCV\", \"LKSX\", \"RKSX\",\n",
    "        \"WOMADLL\", \"WOMADLR\", \"WOMKPL\", \"WOMKPR\", \"WOMSTFL\", \"WOMSTFR\", \"WOMTSL\", \"WOMTSR\",  # WOMAC\n",
    "        \"WPLKN1\", \"WPLKN2\", \"WPLKN3\", \"WPLKN4\", \"WPLKN5\",\n",
    "        \"WPRKN1\", \"WPRKN2\", \"WPRKN3\", \"WPRKN4\", \"WPRKN5\",\n",
    "        \"P01SVLKOST\", \"P01SVRKOST\",\n",
    "        \"KRSR12\", \"KRSL12\",  # knee replacement since last visit, right and left\n",
    "    ]\n",
    "    df = df.loc[:, sel]\n",
    "    return df\n",
    "\n",
    "\n",
    "def print_unique(df, fields):\n",
    "    for field in fields:\n",
    "        print(field, pd.unique(df[field]))\n",
    "\n",
    "\n",
    "def preproc_clinical(df):\n",
    "    # Harmonize the values and fill the missing\n",
    "    for field in (\"PMLKRCV\", \"PMRKRCV\"):\n",
    "        dict_fix = {\n",
    "            np.nan: -1, ' ': -1,\n",
    "            '.: Missing Form/Incomplete Workbook': -1,\n",
    "            0.0: 0, '0': 0, '0: No pain': 0,\n",
    "            1.0: 1, '1': 1, '1: 1': 1,\n",
    "            2.0: 2, '2': 2, '2: 2': 2,\n",
    "            3.0: 3, '3': 3, '3: 3': 3,\n",
    "            4.0: 4, '4': 4, '4: 4': 4,\n",
    "            5.0: 5, '5': 5, '5: 5': 5,\n",
    "            6.0: 6, '6': 6, '6: 6': 6,\n",
    "            7.0: 7, '7': 7, '7: 7': 7,\n",
    "            8.0: 8, '8': 8, '8: 8': 8,\n",
    "            9.0: 9, '9': 9, '9: 9': 9,\n",
    "            10.0: 10, '10': 10, '10: Pain as bad as you can imagine': 10,\n",
    "        }\n",
    "        df = df.fillna({field: -1}, axis=0)\n",
    "        df = df.replace({field: dict_fix})\n",
    "\n",
    "    for field in (\"KPL30CV\", \"KPR30CV\", \"KRSL12\", \"KRSR12\",\n",
    "                  \"P01INJL\", \"P01INJR\", \"P01KSURGL\", \"P01KSURGR\",\n",
    "                  \"P01KRSL\", \"P01KRSR\",\n",
    "                  \"P01ARTL\", \"P01ARTR\", \"P01ARTLINJ\", \"P01ARTRINJ\",\n",
    "                  \"P01MENL\", \"P01MENR\", \"P01MENLINJ\", \"P01MENRINJ\",\n",
    "                  \"P01LRL\", \"P01LRR\",\n",
    "                  \"P01OTSURGL\", \"P01OTSURGR\", \"P01OTSLINJ\", \"P01OTSRINJ\",\n",
    "                 ):\n",
    "        dict_fix = {\n",
    "            '1: Yes': 1, 1.0: 1, '1': 1,\n",
    "            '0: No': 0, 0.0: 0, '0': 0,\n",
    "            '.: Missing Form/Incomplete Workbook': -1, ' ': -1, np.nan: -1,\n",
    "        }\n",
    "        df = df.fillna({field: -1}, axis=0)\n",
    "        df = df.replace({field: dict_fix})\n",
    "\n",
    "    for field in (\"WOMADLL\", \"WOMADLR\", \"WOMKPL\", \"WOMKPR\",\n",
    "                  \"WOMSTFL\", \"WOMSTFR\", \"WOMTSL\", \"WOMTSR\"):\n",
    "        df = df.replace({field: {\" \": -1, np.nan: -1}})\n",
    "        df = df.fillna({field: -1}, axis=0)\n",
    "        df = df.astype({field: float})\n",
    "        \n",
    "    # Normalize the value ranges\n",
    "    for field in (\"WOMKPL\", \"WOMKPR\"):  # WOMAC pain, range [-1, 0-20] -> [-1, 0-100]\n",
    "        df.loc[:, field] = df[field].apply(lambda x: x * 5 if x > 0 else x)\n",
    "\n",
    "    # Melt \"side\"-specific columns\n",
    "    df = pd.concat([df.assign(**{\"side\": \"LEFT\"}),\n",
    "                    df.assign(**{\"side\": \"RIGHT\"})],\n",
    "                    axis=\"index\",\n",
    "                    ignore_index=True)\n",
    "\n",
    "    for f_left, f_right, f_out in [\n",
    "        (\"PMLKRCV\", \"PMRKRCV\", \"PM-KRCV\"),\n",
    "        (\"KPL30CV\", \"KPR30CV\", \"KP-30CV\"),\n",
    "        \n",
    "        (\"WOMADLL\", \"WOMADLR\", \"WOMADL-\"),  # WOMAC physical disability, range [-1, 0-68]\n",
    "        (\"WOMKPL\", \"WOMKPR\", \"WOMKP-\"),  # WOMAC pain\n",
    "        (\"WOMSTFL\", \"WOMSTFR\", \"WOMSTF-\"),  # WOMAC stiffness, range [-1, 0-8]\n",
    "        (\"WOMTSL\", \"WOMTSR\", \"WOMTS-\"),  # WOMAC total score, range [-1, 0-96]\n",
    "        (\"KRSL12\", \"KRSR12\", \"KRS-12\"),  # knee replacement surgery\n",
    "        \n",
    "        (\"P01INJL\", \"P01INJR\", \"P01INJ-\"),  # injury with loss of ability to walk\n",
    "        (\"P01KSURGL\", \"P01KSURGR\", \"P01KSURG-\"),  # surgery or arthroscopy\n",
    "        (\"P01KRSL\", \"P01KRSR\", \"P01KRS-\"),  # part or whole joint replacement\n",
    "        \n",
    "        (\"P01ARTL\", \"P01ARTR\", \"P01ART-\"),  # arthroscopy\n",
    "        (\"P01ARTLINJ\", \"P01ARTRINJ\", \"P01ART-INJ\"),  # arthroscopy after injury\n",
    "        (\"P01MENL\", \"P01MENR\", \"P01MEN-\"),  # meniscectomy\n",
    "        (\"P01MENLINJ\", \"P01MENRINJ\", \"P01MEN-INJ\"),  # meniscectomy after injury\n",
    "\n",
    "        (\"P01LRL\", \"P01LRR\", \"P01LR-\"),  # ligament repair surgery\n",
    "        (\"P01OTSURGL\", \"P01OTSURGR\", \"P01OTSURG-\"),  # any other surgery\n",
    "        (\"P01OTSLINJ\", \"P01OTSRINJ\", \"P01OTS-INJ\"),  # any other surgery after injury\n",
    "    ]:\n",
    "        df = df.assign(**{f_out: \"\"})\n",
    "\n",
    "        df.loc[df[\"side\"] == \"LEFT\", f_out] = df[f_left]\n",
    "        df.loc[df[\"side\"] == \"RIGHT\", f_out] = df[f_right]\n",
    "\n",
    "        df = df.astype({f_out: df[f_left].dtype})\n",
    "        df = df.drop(columns=[f_left, f_right])\n",
    "    # ---\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "dir_clinical = Path(DIR_DATA_ROOT, \"OAI_general/OAI_CompleteData_ASCII\")\n",
    "paths_clinical = sorted(dir_clinical.glob(\"AllClinical??.txt\"))\n",
    "\n",
    "df_clinical = read_compose_clinical(paths_clinical)\n",
    "\n",
    "df_clinical = preproc_clinical(df_clinical)\n",
    "print(len(df_clinical))\n",
    "df_clinical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_enrollees(path):\n",
    "    df = pd.read_csv(path, index_col=False, sep=\"|\", dtype={\"ID\": str, })\n",
    "\n",
    "    df = df.rename(columns={\"ID\": \"patient\"})\n",
    "\n",
    "    df = df.replace({\"P02SEX\": {\"1: Male\": \"MALE\", \"2: Female\": \"FEMALE\"}})\n",
    "    \n",
    "    sel = [\"patient\", \"P02SEX\", \"P02RACE\", \"V00SITE\"]\n",
    "    df = df.loc[:, sel]\n",
    "    return df\n",
    "\n",
    "\n",
    "dir_enrollees = Path(DIR_DATA_ROOT, \"OAI_general/OAI_CompleteData_ASCII\")\n",
    "paths_enrollees = Path(dir_enrollees, \"Enrollees.txt\")\n",
    "\n",
    "df_enrollees = read_enrollees(paths_enrollees)\n",
    "print(len(df_enrollees))\n",
    "df_enrollees.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_compose_xr_sq(paths):\n",
    "    df = read_compose_asmts(paths, capitalize=True)\n",
    "    \n",
    "    df = df.rename(columns={\"ID\": \"patient\",\n",
    "                            \"SIDE\": \"side\", })\n",
    "    \n",
    "    df = df.astype({\"patient\": str})\n",
    "    \n",
    "    df.loc[:, \"side\"] = [side_code_to_str[c] for c in df[\"side\"].tolist()]\n",
    "    df.loc[:, \"visit_month\"] = [prefix_var_to_visit_month[p]\n",
    "                                for p in df[\"PREFIX_VAR\"].tolist()]\n",
    "    df.loc[:, \"visit\"] = [int(p[:-1]) for p in df[\"visit_month\"].tolist()]\n",
    "    \n",
    "    sel = [\"patient\", \"side\", \"PREFIX_VAR\", \"visit_month\", \"visit\",\n",
    "           \"XRKL\",\n",
    "           \"XROSFL\", \"XROSFM\",  # osteophytes, femur (OARSI grades)\n",
    "           \"XROSTL\", \"XROSTM\",  # osteophytes, tibia (OARSI grades)\n",
    "           \"XRJSL\", \"XRJSM\",  # joint-space narrowing (OARSI grades)\n",
    "           \"XRSCFL\", \"XRSCFM\",  # sclerosis, femur (OARSI grades)\n",
    "           \"XRSCTL\", \"XRSCTM\",  # sclerosis, tibia (OARSI grades)\n",
    "           \"XRATTL\", \"XRATTM\",  # attrition (OARSI grades)\n",
    "          ]\n",
    "    df = df.loc[:, sel]\n",
    "    return df\n",
    "\n",
    "\n",
    "def preproc_xr_sq(df):\n",
    "    for field in (\"XRKL\", ):\n",
    "        #print(field, pd.unique(df[field]))\n",
    "        dict_fix = {\n",
    "            \"P\": 5,  # .P - data missing due to a prosthesis/knee replacement\n",
    "            \"T\": -1,  # .T - data missing due to technical reasons (e.g. poor image quality)\n",
    "            #.A - data not expected (e.g.: some V00XR...may be missing if participant has KLG<2 in both knees at all time points)\n",
    "            np.nan: -1,\n",
    "        }\n",
    "        #df = df.fillna({field: -1}, axis=0)\n",
    "        df = df.replace({field: dict_fix})\n",
    "\n",
    "    for field in (\"XROSFL\", \"XROSFM\", \"XROSTL\", \"XROSTM\",\n",
    "                  \"XRJSL\", \"XRJSM\",\n",
    "                  \"XRSCFL\", \"XRSCFM\", \"XRSCTL\", \"XRSCTM\",\n",
    "                  \"XRATTL\", \"XRATTM\"):\n",
    "        #print(field, pd.unique(df[field]))\n",
    "        #df = df.fillna({field: -1}, axis=0)\n",
    "        dict_fix = {\"P\": -1, \"T\": -1, \"A\": -1, \"M\": -1, np.nan: -1}\n",
    "        df = df.replace({field: dict_fix})\n",
    "\n",
    "    fields = (\"XRKL\",\n",
    "              \"XROSFL\", \"XROSFM\", \"XROSTL\", \"XROSTM\",\n",
    "              \"XRSCFL\", \"XRSCFM\", \"XRSCTL\", \"XRSCTM\",\n",
    "              \"XRATTL\", \"XRATTM\")\n",
    "    df = df.astype({f: int for f in fields})\n",
    "\n",
    "    fields = (\"XRJSL\", \"XRJSM\")\n",
    "    df = df.astype({f: float for f in fields})\n",
    "    \n",
    "    # Keep only one assessment for (subject, knee, follow-up)\n",
    "    df = df.drop_duplicates(subset=[\"patient\", \"side\", \"visit\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Post-20210721\n",
    "dir_xr_sq = Path(DIR_DATA_ROOT, \"OAI_general/OAI_CompleteData_SAS\")\n",
    "paths_xr_sq = sorted(dir_xr_sq.glob(\"kxr_sq_bu??.sas7bdat\"))\n",
    "\n",
    "df_xr_sq = read_compose_xr_sq(paths_xr_sq)\n",
    "\n",
    "df_xr_sq = preproc_xr_sq(df_xr_sq)\n",
    "print(len(df_xr_sq))\n",
    "df_xr_sq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_compose_outcomes(paths):\n",
    "    df = read_compose_asmts(paths)\n",
    "\n",
    "    df.columns = df.columns.str.upper()\n",
    "    df = df.rename(columns={\"ID\": \"patient\", })\n",
    "    \n",
    "    df = df.astype({\"patient\": str})\n",
    "    \n",
    "#     print(df.columns)\n",
    "    sel = [\n",
    "        \"patient\",\n",
    "        \"ELKVSRP\",  # OAI visit follow-up TKR (left knee) self-reported at\n",
    "        \"ERKVSRP\",  # OAI visit follow-up TKR (right knee) self-reported at\n",
    "        \n",
    "        \"ELKXRAF\",  # closest OAI visit with knee XR after follow-up TKR (left knee)\n",
    "        \"ERKXRAF\",  # closest OAI visit with knee XR after follow-up TKR (right knee)\n",
    "        \n",
    "        \"ELKXRPR\",  # closest OAI visit with knee XR prior to follow-up TKR (left knee)\n",
    "        \"ERKXRPR\",  # closest OAI visit with knee XR prior to follow-up TKR (right knee)\n",
    "        \n",
    "#         \"ELKTLPR\",  # total or partial follow-up knee replacement (left knee)\n",
    "#         \"ERKTLPR\",  # total or partial follow-up knee replacement (right knee)\n",
    "        \n",
    "        \"ELKBLRP\",  # knee replacement (right knee) seen on baseline OAI XR\n",
    "        \"ERKBLRP\",  # knee replacement (right knee) seen on baseline OAI XR\n",
    "         \n",
    "        \"ELKRPSN\",  # knee replacement (left knee) seen on follow-up OAI XR\n",
    "        \"ERKRPSN\",  # knee replacement (right knee) seen on follow-up OAI XR\n",
    "    ]\n",
    "    df = df.loc[:, sel]\n",
    "    return df\n",
    "\n",
    "\n",
    "def preproc_outcomes(df):\n",
    "    # Harmonize the values and fill the missing\n",
    "    for field in (\"ELKVSRP\", \"ERKVSRP\",\n",
    "                  \"ELKXRAF\", \"ERKXRAF\",\n",
    "                  \"ELKXRPR\", \"ERKXRPR\",\n",
    "                 ):\n",
    "        dict_fix = {\n",
    "            \"0: Baseline\": 0,\n",
    "            \"1: 12-month\": 12,\n",
    "            \"2: 18-month\": 18,\n",
    "            \"3: 24-month\": 24,\n",
    "            \"4: 30-month\": 30,\n",
    "            \"5: 36-month\": 36,\n",
    "            \"6: 48-month\": 48,\n",
    "            \"7: 60-month\": 60,\n",
    "            \"8: 72-month\": 72,\n",
    "            \"9: 84-month\": 84,\n",
    "            \"10: 96-month\": 96,\n",
    "            \"11: 108-month\": 108,\n",
    "            '.: Missing Form/Incomplete Workbook': -1,\n",
    "            np.nan: -1, ' ': -1,\n",
    "        }\n",
    "        df = df.fillna({field: -1}, axis=0)\n",
    "        df = df.replace({field: dict_fix})\n",
    "    \n",
    "    for field in (\"ELKBLRP\", \"ERKBLRP\"):\n",
    "        dict_fix = {\n",
    "            \"0: No\": 0,\n",
    "            \"1: Yes\": 1,            \n",
    "            '.: Missing Form/Incomplete Workbook': -1,\n",
    "        }\n",
    "        df = df.fillna({field: -1}, axis=0)\n",
    "        df = df.replace({field: dict_fix})\n",
    "    \n",
    "    for field in (\"ELKRPSN\", \"ERKRPSN\"):\n",
    "        dict_fix = {\n",
    "            '.: Missing Form/Incomplete Workbook': -1,\n",
    "            '0: No replacement seen on any FU xray': 0,\n",
    "            '1: Yes, replacement seen on FU xray': 1,\n",
    "            '2: No FU xrays of this knee (or hip)': 2,\n",
    "        }\n",
    "        df = df.fillna({field: -1}, axis=0)\n",
    "        df = df.replace({field: dict_fix})\n",
    "        \n",
    "    # Melt \"side\"-specific columns\n",
    "    df = pd.concat([df.assign(**{\"side\": \"LEFT\"}),\n",
    "                    df.assign(**{\"side\": \"RIGHT\"})],\n",
    "                    axis=\"index\",\n",
    "                    ignore_index=True)\n",
    "\n",
    "    for f_left, f_right, f_out in [\n",
    "        (\"ELKVSRP\", \"ERKVSRP\", \"E-KVSRP\"),\n",
    "        (\"ELKXRAF\", \"ERKXRAF\", \"E-KXRAF\"),\n",
    "        (\"ELKXRPR\", \"ERKXRPR\", \"E-KXRPR\"),\n",
    "#         (\"ELKTLPR\", \"ERKTLPR\", \"E-KTLPR\"),\n",
    "        (\"ELKBLRP\", \"ERKBLRP\", \"E-KBLRP\"),\n",
    "        (\"ELKRPSN\", \"ERKRPSN\", \"E-KRPSN\"),\n",
    "    ]:\n",
    "        df = df.assign(**{f_out: \"\"})\n",
    "\n",
    "        df.loc[df[\"side\"] == \"LEFT\", f_out] = df[f_left]\n",
    "        df.loc[df[\"side\"] == \"RIGHT\", f_out] = df[f_right]\n",
    "\n",
    "        df = df.astype({f_out: df[f_left].dtype})\n",
    "        df = df.drop(columns=[f_left, f_right])\n",
    "    # ---\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "dir_outcomes = Path(DIR_DATA_ROOT, \"OAI_general/OAI_CompleteData_ASCII\")\n",
    "paths_outcomes = sorted(dir_outcomes.glob(\"Outcomes99.txt\"))\n",
    "\n",
    "df_outcomes = read_compose_outcomes(paths_outcomes)\n",
    "\n",
    "df_outcomes = preproc_outcomes(df_outcomes)\n",
    "print(len(df_outcomes))\n",
    "df_outcomes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_preproc_tiulpin2019(path):\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    df = df.rename(columns={\"ID\": \"patient\",\n",
    "                            \"Side\": \"side\",\n",
    "                            \"Prog_increase\": \"tiulpin2019_kl_diff\",\n",
    "                            \"Progressor\": \"tiulpin2019_prog\"})\n",
    "\n",
    "    df = df.astype({\"patient\": str})\n",
    "    \n",
    "    for field in (\"side\",):\n",
    "        dict_fix = {\n",
    "            \"L\": \"LEFT\",\n",
    "            \"R\": \"RIGHT\",\n",
    "        }\n",
    "        df = df.replace({field: dict_fix})\n",
    "\n",
    "    df[\"visit\"] = 0\n",
    "    # ---\n",
    "    sel = [\"patient\", \"side\", \"visit\", \"tiulpin2019_kl_diff\", \"tiulpin2019_prog\"]\n",
    "    df = df.loc[:, sel]\n",
    "    return df\n",
    "\n",
    "path_tiulpin2019 = Path(DIR_DATA_ROOT, \"tiulpin2019multimodal__labels.csv\")\n",
    "\n",
    "df_tiulpin2019 = read_preproc_tiulpin2019(path_tiulpin2019)\n",
    "\n",
    "print(len(df_tiulpin2019))\n",
    "df_tiulpin2019.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Merge the meta info"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Records _enrollees: {len(df_enrollees)}\")\n",
    "print(f\"Records _clinical: {len(df_clinical)}\")\n",
    "print(f\"Records _xr_sq: {len(df_xr_sq)}\")\n",
    "print(f\"Records _outcomes: {len(df_outcomes)}\")\n",
    "# print(f\"Records _contents: {len(df_contents)}\")\n",
    "print()\n",
    "\n",
    "df_t = df_enrollees.copy()\n",
    "t_num_uniq_pat = len(df_t.drop_duplicates(subset=[\"patient\",]))\n",
    "print(f\"Records merge (+enrollees): {len(df_t)}, \"\n",
    "      f\"subjects: {t_num_uniq_pat}\")\n",
    "\n",
    "df_t = df_t.merge(df_clinical,\n",
    "                  on=\"patient\",\n",
    "                  how=\"inner\")\n",
    "t_num_uniq_pat = len(df_t.drop_duplicates(subset=[\"patient\",]))\n",
    "t_num_uniq_knees = len(df_t.drop_duplicates(subset=[\"patient\", \"side\"]))\n",
    "print(f\"Records merge (+clinical): {len(df_t)}, \"\n",
    "      f\"subjects: {t_num_uniq_pat}, knees: {t_num_uniq_knees}\")\n",
    "\n",
    "df_t = df_t.merge(df_xr_sq,\n",
    "                  on=[\"patient\", \"side\", \"visit\", \"visit_month\", \"PREFIX_VAR\"],\n",
    "                  how=\"inner\")\n",
    "t_num_uniq_pat = len(df_t.drop_duplicates(subset=[\"patient\",]))\n",
    "t_num_uniq_knees = len(df_t.drop_duplicates(subset=[\"patient\", \"side\"]))\n",
    "print(f\"Records merge (+xr_sq): {len(df_t)}, \"\n",
    "      f\"subjects: {t_num_uniq_pat}, knees: {t_num_uniq_knees}\")\n",
    "\n",
    "df_t = df_t.merge(df_outcomes,\n",
    "                  on=[\"patient\", \"side\"],\n",
    "                  how=\"left\")\n",
    "t_num_uniq_pat = len(df_t.drop_duplicates(subset=[\"patient\",]))\n",
    "t_num_uniq_knees = len(df_t.drop_duplicates(subset=[\"patient\", \"side\"]))\n",
    "print(f\"Records merge (+outcomes): {len(df_t)}, \"\n",
    "      f\"subjects: {t_num_uniq_pat}, knees: {t_num_uniq_knees}\")\n",
    "\n",
    "df_merge = df_t\n",
    "display(df_merge.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in prior art\n",
    "print(f\"Records _merge: {len(df_merge)}\")\n",
    "print(f\"Records _tiulpin2019: {len(df_tiulpin2019)}\")\n",
    "print()\n",
    "\n",
    "df_t = df_merge.copy()\n",
    "print(f\"Records merge: {len(df_t)}, \"\n",
    "      f\"subjects: {len(pd.unique(df_t['patient']))}\")\n",
    "\n",
    "df_t = df_t.merge(df_tiulpin2019,\n",
    "                  on=[\"patient\", \"side\", \"visit\"],\n",
    "                  how=\"left\",\n",
    "                  indicator=\"tiulpin2019_sel\")\n",
    "dict_fix = {\"both\": 1, \"left_only\": 0}  # omitted due to `left` merge - \"right_only\": 1, \n",
    "df_t = df_t.replace({\"tiulpin2019_sel\": dict_fix})\n",
    "print(f\"Records merge (+tiulpin2019): {len(df_t)}, \"\n",
    "      f\"subjects: {len(pd.unique(df_t['patient']))}\")\n",
    "\n",
    "# Fill missing with -1 to ease downstream processing\n",
    "for field in (\"tiulpin2019_kl_diff\", \"tiulpin2019_prog\"):\n",
    "    df_t = df_t.replace({field: {\" \": -1, np.nan: -1}})\n",
    "    df_t = df_t.fillna({field: -1}, axis=0)\n",
    "    df_t = df_t.astype({field: int})\n",
    "\n",
    "df_merge = df_t\n",
    "display(df_merge.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Select the subjects, define the targets"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crit_prog_kl_from_to_panfilov(d, visit_to, visit_from=0):\n",
    "    \"\"\"\n",
    "    - worsening in KL grade, excluding KL0->KL1\n",
    "    \"\"\"\n",
    "    indicator = True\n",
    "    criterion = -1\n",
    "\n",
    "    d = d.sort_values(by=\"visit\", axis=\"index\").copy()\n",
    "    \n",
    "    # Exclude missing records\n",
    "    d = d[d[\"XRKL\"] != -1]\n",
    "\n",
    "    visits_avail = d[\"visit\"].tolist()\n",
    "    \n",
    "    # Subject is present at baseline\n",
    "    indicator &= (visit_from in visits_avail)\n",
    "    if not indicator: return indicator, criterion, \"0: not_present_at_baseline\"\n",
    "    \n",
    "    # No KL=4 at baseline\n",
    "    indicator &= d[d[\"visit\"] == visit_from][\"XRKL\"].iloc[0] != 4\n",
    "    if not indicator: return indicator, criterion, \"1: KLG4_at_baseline\"\n",
    "    # No TKR at baseline\n",
    "    indicator &= d[d[\"visit\"] == visit_from][\"XRKL\"].iloc[0] != 5\n",
    "    if not indicator: return indicator, criterion, \"2: TKR_at_baseline\"\n",
    "\n",
    "    # Exclude TKR records\n",
    "    d = d[d[\"XRKL\"] != 5]\n",
    "\n",
    "    # Merge KL0 and KL1\n",
    "    d_m = d.copy()\n",
    "    d_m.loc[d[\"XRKL\"] == 0, \"XRKL\"] = 1\n",
    "\n",
    "    # Consider interval\n",
    "    sel_inter = (d_m[\"visit\"] >= visit_from) & (d_m[\"visit\"] <= visit_to)\n",
    "    visits_inter = d_m[\"visit\"][sel_inter].tolist()\n",
    "    \n",
    "    # At least 1 exam besides baseline\n",
    "    indicator &= len(visits_inter) >= 2\n",
    "    if not indicator: return indicator, criterion, \"3: no_followups\"\n",
    "    \n",
    "    # Persistent non-decrease of KL within interval\n",
    "    t_i = d_m[sel_inter]\n",
    "    indicator &= np.all(np.diff(t_i[\"XRKL\"].to_numpy()) >= 0)\n",
    "    if not indicator: return indicator, criterion, \"4: KLG_decrease\"\n",
    "\n",
    "    if (visit_to in visits_inter) and \\\n",
    "        (d_m.loc[d[\"visit\"] == visit_to, \"XRKL\"].values[0] == \\\n",
    "         d_m.loc[d[\"visit\"] == visit_from, \"XRKL\"].values[0]):\n",
    "        # Present and not progressed at end of interval\n",
    "        indicator &= True\n",
    "        criterion = 0\n",
    "        reason = \"9: ok\"\n",
    "    elif np.any(np.diff(t_i[\"XRKL\"].to_numpy()) > 0):\n",
    "        # Progressed before/at end of interval\n",
    "        indicator &= True\n",
    "        criterion = 1\n",
    "        reason = \"9: ok\"\n",
    "    else:\n",
    "        indicator &= False\n",
    "        criterion = -1\n",
    "        reason = \"5: insufficient_followups\"\n",
    "        \n",
    "    return indicator, criterion, reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create progressions labels\n",
    "df_t = df_merge.copy()\n",
    "\n",
    "dict_labels = defaultdict(list)\n",
    "\n",
    "# t_iter = 0\n",
    "for gb_name, gb_df in tqdm(df_t.groupby([\"patient\", \"side\"], sort=False)):\n",
    "    dict_labels[\"patient\"].append(gb_name[0])\n",
    "    dict_labels[\"side\"].append(gb_name[1])\n",
    "    # Derived for baseline data only\n",
    "    dict_labels[\"visit\"].append(0)\n",
    "\n",
    "    t_12 = crit_prog_kl_from_to_panfilov(gb_df, visit_to=12, visit_from=0)\n",
    "    t_24 = crit_prog_kl_from_to_panfilov(gb_df, visit_to=24, visit_from=0)\n",
    "    t_36 = crit_prog_kl_from_to_panfilov(gb_df, visit_to=36, visit_from=0)\n",
    "    t_48 = crit_prog_kl_from_to_panfilov(gb_df, visit_to=48, visit_from=0)\n",
    "    t_72 = crit_prog_kl_from_to_panfilov(gb_df, visit_to=72, visit_from=0)\n",
    "    t_96 = crit_prog_kl_from_to_panfilov(gb_df, visit_to=96, visit_from=0)\n",
    "\n",
    "    dict_labels[\"panfilov_sel_kl_12\"].append(int(t_12[0]))\n",
    "    dict_labels[\"panfilov_sel_kl_24\"].append(int(t_24[0]))\n",
    "    dict_labels[\"panfilov_sel_kl_36\"].append(int(t_36[0]))\n",
    "    dict_labels[\"panfilov_sel_kl_48\"].append(int(t_48[0]))\n",
    "    dict_labels[\"panfilov_sel_kl_72\"].append(int(t_72[0]))\n",
    "    dict_labels[\"panfilov_sel_kl_96\"].append(int(t_96[0]))\n",
    "    \n",
    "    dict_labels[\"prog_kl_12\"].append(t_12[1])\n",
    "    dict_labels[\"prog_kl_24\"].append(t_24[1])\n",
    "    dict_labels[\"prog_kl_36\"].append(t_36[1])\n",
    "    dict_labels[\"prog_kl_48\"].append(t_48[1])\n",
    "    dict_labels[\"prog_kl_72\"].append(t_72[1])\n",
    "    dict_labels[\"prog_kl_96\"].append(t_96[1])\n",
    "    \n",
    "    dict_labels[\"reason_kl_12\"].append(t_12[2])\n",
    "    dict_labels[\"reason_kl_24\"].append(t_24[2])\n",
    "    dict_labels[\"reason_kl_36\"].append(t_36[2])\n",
    "    dict_labels[\"reason_kl_48\"].append(t_48[2])\n",
    "    dict_labels[\"reason_kl_72\"].append(t_72[2])\n",
    "    dict_labels[\"reason_kl_96\"].append(t_96[2])\n",
    "    \n",
    "#     t_iter += 1\n",
    "#     if t_iter > 10:\n",
    "#         break\n",
    "\n",
    "df_labels = pd.DataFrame.from_dict(dict_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclusions are in this order\n",
    "exclusion_reasons_order = (\n",
    "    \"0: not_present_at_baseline\",\n",
    "    \"1: KLG4_at_baseline\",\n",
    "    \"2: TKR_at_baseline\",\n",
    "    \"3: no_followups\",\n",
    "    \"4: KLG_decrease\",\n",
    "    \"5: insufficient_followups\",\n",
    "#     \"9: ok\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_reasons = (\n",
    "    \"reason_kl_12\",\n",
    "    \"reason_kl_24\",\n",
    "    \"reason_kl_36\",\n",
    "    \"reason_kl_48\",\n",
    "    \"reason_kl_72\",\n",
    "    \"reason_kl_96\",\n",
    ")\n",
    "\n",
    "for target_reason in target_reasons:\n",
    "    display(f\"---- {target_reason} ----\")\n",
    "    # Init df\n",
    "    t_df = df_labels.copy()\n",
    "    \n",
    "    # Count the number of cases by reason\n",
    "#     display(t_df.groupby(target_reason).count())\n",
    "    display(t_df[target_reason].value_counts().sort_index(ascending=True))\n",
    "    \n",
    "    t_pat = len(t_df.drop_duplicates(subset=[\"patient\",]))\n",
    "    t_knees = len(t_df.drop_duplicates(subset=[\"patient\", \"side\"]))\n",
    "    print(f\"Starting with': {t_pat} subjects, {t_knees} knees\")\n",
    "    \n",
    "    # Count the number of subjects and knee after exclusion decrementally\n",
    "    t_df_to_keep = t_df\n",
    "    \n",
    "    for reason in exclusion_reasons_order:\n",
    "        t_df_to_excl = t_df_to_keep[t_df_to_keep[target_reason] == reason]\n",
    "        t_pat_to_excl = len(t_df_to_excl.drop_duplicates(subset=[\"patient\",]))\n",
    "        t_knees_to_excl = len(t_df_to_excl.drop_duplicates(subset=[\"patient\", \"side\"]))\n",
    "        print(f\"Excluded {t_pat_to_excl:>4d} subjects, {t_knees_to_excl:>4d} knees, as '{reason}'\")\n",
    "        \n",
    "        t_df_to_keep = t_df_to_keep[t_df_to_keep[target_reason] != reason]\n",
    "        t_pat_to_keep = len(t_df_to_keep.drop_duplicates(subset=[\"patient\",]))\n",
    "        t_knees_to_keep = len(t_df_to_keep.drop_duplicates(subset=[\"patient\", \"side\"]))\n",
    "        print(f\"Remaining: {t_pat_to_keep:>4d} subjects, {t_knees_to_keep:>4d} knees\")\n",
    "    \n",
    "    target = target_reason.replace(\"reason\", \"prog\")\n",
    "    #display(t_df_to_keep.groupby(target).count())\n",
    "    \n",
    "    t_df_pos = t_df_to_keep[t_df_to_keep[target] == 1]\n",
    "    t_pos_pat = len(t_df_pos.drop_duplicates(subset=[\"patient\",]))\n",
    "    t_pos_knees = len(t_df_pos.drop_duplicates(subset=[\"patient\", \"side\"]))\n",
    "    \n",
    "    t_df_neg = t_df_to_keep[t_df_to_keep[target] == 0]\n",
    "    t_neg_pat = len(t_df_neg.drop_duplicates(subset=[\"patient\",]))\n",
    "    t_neg_knees = len(t_df_neg.drop_duplicates(subset=[\"patient\", \"side\"]))\n",
    "    \n",
    "    print(f\"Controls: {t_neg_pat:>4d} subjects, {t_neg_knees:>4d} knees\")\n",
    "    print(f\"Cases: {t_pos_pat:>4d} subjects, {t_pos_knees:>4d} knees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save labels to file\n",
    "path_out = Path(DIR_DATA_ROOT, \"prog_labels.csv\")\n",
    "df_labels.to_csv(path_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge meta with labels\n",
    "print(f\"Records _merge: {len(df_merge)}\")\n",
    "print(f\"Records _labels: {len(df_labels)}\")\n",
    "print()\n",
    "\n",
    "df_t = df_merge.copy()\n",
    "print(f\"Records merge: {len(df_t)}, \"\n",
    "      f\"subjects: {len(pd.unique(df_t['patient']))}\")\n",
    "\n",
    "df_t = df_t.merge(df_labels,\n",
    "                  on=[\"patient\", \"side\", \"visit\"],\n",
    "                  how=\"left\")\n",
    "print(f\"Records merge (+labels): {len(df_t)}, \"\n",
    "      f\"subjects: {len(pd.unique(df_t['patient']))}\")\n",
    "\n",
    "# Fill missing with -1 to ease downstream processing\n",
    "for field in (\n",
    "    \"panfilov_sel_kl_12\", \"panfilov_sel_kl_24\", \"panfilov_sel_kl_36\",\n",
    "    \"panfilov_sel_kl_48\", \"panfilov_sel_kl_72\", \"panfilov_sel_kl_96\",\n",
    "    \"prog_kl_12\", \"prog_kl_24\", \"prog_kl_36\",\n",
    "    \"prog_kl_48\", \"prog_kl_72\", \"prog_kl_96\",\n",
    "):\n",
    "    df_t = df_t.replace({field: {\" \": -1, np.nan: -1}})\n",
    "    df_t = df_t.fillna({field: -1}, axis=0)\n",
    "    df_t = df_t.astype({field: int})\n",
    "    \n",
    "# display(df_t.head())\n",
    "df_merge = df_t\n",
    "display(df_merge.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save meta+labels to file\n",
    "dir_out = Path(DIR_DATA_ROOT, \"OAI_Clin_prep\")\n",
    "dir_out.mkdir(parents=True, exist_ok=True)\n",
    "path_out = Path(dir_out, \"meta_base.csv\")\n",
    "df_merge.to_csv(path_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 4. Build index of imaging data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the scans to be copied\n",
    "df_t_asmts = df_merge.copy()\n",
    "df_t_imgs = df_contents_proc.copy()\n",
    "\n",
    "def print_selection_stats(df):\n",
    "    print(\"Records: \", len(df), \", \",\n",
    "          \"knees: \", len(df.drop_duplicates(subset=[\"patient\", \"side\"])), \", \",\n",
    "          \"subjects: \", len(pd.unique(df[\"patient\"])))\n",
    "\n",
    "print(\"Clinical / assessments:\")\n",
    "print_selection_stats(df_t_asmts)\n",
    "# - Only the baseline scans\n",
    "df_t_asmts = df_t_asmts[df_t_asmts[\"visit\"] == 0]\n",
    "print_selection_stats(df_t_asmts)\n",
    "\n",
    "print(\"Imaging:\")\n",
    "df_t_imgs = df_t_imgs.sort_values(by=[\"patient\", \"side\", \"visit\", \"sequence\"])\n",
    "print_selection_stats(df_t_imgs)\n",
    "\n",
    "# - Only the baseline scans\n",
    "print(\"Only baseline:\")\n",
    "df_t_imgs = df_t_imgs[df_t_imgs[\"visit\"] == 0]\n",
    "print_selection_stats(df_t_imgs)\n",
    "\n",
    "# - Only the following imaging protocols\n",
    "# TODO: uncomment one at a time\n",
    "t_sequences = [\n",
    "    \"SAG_3D_DESS\",\n",
    "#     \"COR_IW_TSE\",\n",
    "#     \"SAG_T2_MAP\",       # Incidence Knee only\n",
    "]\n",
    "df_t_imgs = df_t_imgs[df_t_imgs[\"sequence\"].isin(t_sequences)]\n",
    "print_selection_stats(df_t_imgs)\n",
    "\n",
    "# - In case of rescan, exclude all the previous scans\n",
    "print(\"No rescans:\")\n",
    "df_t_imgs = df_t_imgs.drop_duplicates(\n",
    "    subset=[\"patient\", \"visit\", \"side\", \"sequence\"],\n",
    "    keep=\"last\",\n",
    "    ignore_index=True)\n",
    "print_selection_stats(df_t_imgs)\n",
    "\n",
    "\n",
    "print(\"Intersect clinical and imaging:\")\n",
    "df_extract = df_t_imgs.copy()\n",
    "# print(\"Intersect clinical and imaging:\")\n",
    "# selectors = [\"patient\", \"visit_month\", \"visit\", \"side\"]\n",
    "# df_extract = df_t_asmts.merge(df_t_imgs, on=selectors, how=\"inner\")\n",
    "# print_selection_stats(df_extract)\n",
    "# # - Only the knees with _all_ considered protocols\n",
    "# df_extract = (\n",
    "#     df_extract\n",
    "#     .groupby([\"patient\", \"side\"], sort=False)\n",
    "#     .filter(lambda x: all(e in x[\"sequence\"].tolist() for e in t_sequences))\n",
    "# )\n",
    "print_selection_stats(df_extract)\n",
    "\n",
    "# Save to a .csv file\n",
    "# TODO: uncomment one at a time\n",
    "path_out = \"/home/egor/Workspace/p03_koafusion/meta_extract__sag_3d_dess.csv\"\n",
    "# path_out = \"/home/egor/Workspace/p03_koafusion/meta_extract__cor_iw_tse.csv\"\n",
    "# path_out = \"/home/egor/Workspace/p03_koafusion/meta_extract__sag_t2_map.csv\"\n",
    "\n",
    "df_extract.to_csv(path_out, index=False)\n",
    "display(df_extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 5. Extract individual modality data from OAI"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES_TO_COPY = None\n",
    "# NUM_SAMPLES_TO_COPY = 4\n",
    "\n",
    "# TODO: uncomment one at a time\n",
    "SEQUENCES_TO_COPY = [\n",
    "    \"SAG_3D_DESS\",\n",
    "#     \"COR_IW_TSE\",\n",
    "#     \"SAG_T2_MAP\",\n",
    "]\n",
    "\n",
    "df_to_copy = df_extract.copy()\n",
    "print(f\"Init, to copy: {len(df_to_copy)}\")\n",
    "\n",
    "# OPTION: Only one sequence to copy\n",
    "df_to_copy = df_to_copy[df_to_copy[\"sequence\"].isin(SEQUENCES_TO_COPY)]\n",
    "\n",
    "# Select number of samples to copy\n",
    "df_to_copy = df_to_copy.iloc[:NUM_SAMPLES_TO_COPY, :]\n",
    "df_to_copy.head()\n",
    "\n",
    "print(f\"Selected, to copy: {len(df_to_copy)}\")\n",
    "display(df_to_copy.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MR\n",
    "def copy_scans_from_oai(path_root_source, path_root_target, df, n_jobs=1, dry_run=False):\n",
    "    def silent_copy(p_from, p_to, dry_run):\n",
    "        if not p_from.exists():\n",
    "            print(f\"Missing: {p_from}\")\n",
    "            return False\n",
    "        else:\n",
    "            if not dry_run:\n",
    "                shutil.copytree(p_from, p_to)\n",
    "            return True\n",
    "        \n",
    "    from joblib import delayed, Parallel\n",
    "    \n",
    "    tasks = []\n",
    "    print(f\"Total: {len(df)}\")\n",
    "    \n",
    "    for _, r in df.iterrows():\n",
    "        path_tmp_from = Path(path_root_source,\n",
    "                             r['visit_month'][1:],\n",
    "                             r['Folder'])\n",
    "        path_tmp_to = Path(path_root_target,\n",
    "                           r['visit_month'][1:],\n",
    "                           r['Folder'])\n",
    "        \n",
    "        tasks.append(delayed(silent_copy)(path_tmp_from, path_tmp_to, dry_run))\n",
    "\n",
    "    ret = Parallel(n_jobs=n_jobs, verbose=5)(t for t in tasks)\n",
    "    # Exclude missing or erroneous data\n",
    "    return df.loc[ret, :]\n",
    "\n",
    "path_root_source = Path(DIR_DATA_ROOT, \"OAIBaselineImages\")\n",
    "# TODO: uncomment one at a time\n",
    "path_root_target = Path(DIR_DATA_ROOT, \"OAI_SAG_3D_DESS_raw\")\n",
    "# path_root_target = Path(DIR_DATA_ROOT, \"OAI_COR_IW_TSE_raw\")\n",
    "# path_root_target = Path(DIR_DATA_ROOT, \"OAI_SAG_T2_MAP_raw\")\n",
    "\n",
    "os.makedirs(path_root_target, exist_ok=True)\n",
    "\n",
    "df_copied = copy_scans_from_oai(path_root_source=path_root_source,\n",
    "                                path_root_target=path_root_target,\n",
    "                                df=df_to_copy,\n",
    "                                n_jobs=4,\n",
    "#                                 dry_run=True,\n",
    "                               )\n",
    "\n",
    "df_copied.to_csv(Path(path_root_target, 'meta_base.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
